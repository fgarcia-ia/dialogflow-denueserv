# Gu√≠a para Desplegar Procesos de Datos a Cloud Run

## üìã Introducci√≥n

Esta gu√≠a te ayudar√° a convertir tus notebooks de Jupyter (`.ipynb`) en servicios de Cloud Run de Google Cloud Platform (GCP). Cloud Run permite ejecutar tus procesos de datos de forma automatizada y escalable sin necesidad de mantener servidores.

### ¬øQu√© es Cloud Run?
Cloud Run es un servicio de GCP que ejecuta c√≥digo Python como una API web. Cuando alguien hace una petici√≥n HTTP (POST), tu c√≥digo se ejecuta y devuelve una respuesta en formato JSON.

### ¬øPor qu√© usar Cloud Run?
- ‚úÖ **Automatizaci√≥n**: Puedes programar ejecuciones autom√°ticas
- ‚úÖ **Escalabilidad**: Se adapta autom√°ticamente a la carga de trabajo
- ‚úÖ **Sin servidores**: No necesitas mantener infraestructura
- ‚úÖ **Integraci√≥n**: Se integra f√°cilmente con otros servicios de GCP

---

## üìÅ Estructura de Archivos

Antes de comenzar, es importante entender qu√© hace cada archivo en la plantilla:

```
api_plantilla/
‚îú‚îÄ‚îÄ main.py              # C√≥digo principal que se ejecuta en Cloud Run
‚îú‚îÄ‚îÄ local_main.py         # Versi√≥n local para pruebas (usa Flask)
‚îú‚îÄ‚îÄ requirements.txt      # Librer√≠as de Python necesarias
‚îú‚îÄ‚îÄ Tools.py              # Funciones auxiliares (BigQuery, Storage, etc.)
‚îú‚îÄ‚îÄ local_env.yaml        # Variables de entorno para pruebas locales
‚îú‚îÄ‚îÄ local_test.sh         # Script para probar el servicio localmente
‚îî‚îÄ‚îÄ local_readme.md       # Esta gu√≠a
```

### Descripci√≥n de Archivos

- **`main.py`**: Contiene la l√≥gica principal de tu proceso. Es el archivo que Cloud Run ejecutar√°.
- **`local_main.py`**: Permite ejecutar y probar tu c√≥digo localmente antes de desplegar.
- **`requirements.txt`**: Lista todas las librer√≠as de Python que necesita tu c√≥digo (pandas, numpy, etc.).
- **`Tools.py`**: Funciones reutilizables para trabajar con BigQuery, Cloud Storage y otras herramientas de GCP.
- **`local_env.yaml`**: Variables de configuraci√≥n (rutas de archivos, IDs de proyectos, etc.).
- **`local_test.sh`**: Script para hacer pruebas HTTP al servicio local.

---

## üöÄ Proceso Paso a Paso

### Paso 0: Preparaci√≥n

#### Requisitos Previos
- ‚úÖ Notebook de Jupyter con la l√≥gica del proceso funcionando
- ‚úÖ Ubuntu como sistema operativo (o WSL en Windows, tambien puede ser vertex notebook o collab)
- ‚úÖ Python 3.12 instalado
- ‚úÖ Acceso a GCP con permisos para Cloud Run
- ‚úÖ `gcloud` CLI instalado y configurado

#### Crear una Nueva Carpeta para tu Servicio

1. Copia la carpeta `api_plantilla` y ren√≥mbrala con el nombre de tu servicio:
   ```bash
   cp -r api_plantilla api_mi_proceso
   cd api_mi_proceso
   ```

2. El nombre debe ser descriptivo y seguir el formato `api_nombre_proceso` (ej: `api_carga_presupuesto`, `api_procesamiento_datos`)

---

### Paso 1: Convertir la L√≥gica del Notebook a `main.py`

#### 1.1 Entender la Estructura

El archivo `main.py` tiene una estructura b√°sica:

```python
import os
from flask import jsonify
import functions_framework

@functions_framework.http
def functionRun(request):
    """Funci√≥n principal para ejecutar el pipeline"""
    
    # TODO: Aqu√≠ va tu l√≥gica del notebook
    
    return jsonify({"message": "executed successfully", "data": None})
```

#### 1.2 Pasos para Migrar tu Notebook

1. **Abre tu notebook** y revisa las celdas principales
2. **Identifica las funciones principales** que realizan el trabajo
3. **Copia el c√≥digo relevante** al archivo `main.py`
4. **Elimina c√≥digo de visualizaci√≥n** (matplotlib, print de debugging, etc.)
5. **Mant√©n solo la l√≥gica de procesamiento**

#### 1.3 Ejemplo de Migraci√≥n

**Notebook Original:**
```python
# Celda 1: Cargar datos
import pandas as pd
df = pd.read_excel("archivo.xlsx")

# Celda 2: Procesar
df['nueva_columna'] = df['columna1'] * 2

# Celda 3: Guardar
df.to_csv("resultado.csv")
print("Proceso completado")
```

**Versi√≥n en `main.py`:**
```python
import os
from flask import jsonify
import pandas as pd
import functions_framework
from Tools import upload_to_bigquery  # Funci√≥n auxiliar

@functions_framework.http
def functionRun(request):
    """Funci√≥n principal para ejecutar el pipeline"""
    try:
        # 1. Cargar datos
        archivo = os.getenv('ARCHIVO_EXCEL')
        df = pd.read_excel(archivo)
        
        # 2. Procesar
        df['nueva_columna'] = df['columna1'] * 2
        
        # 3. Guardar en BigQuery
        upload_to_bigquery(
            df,
            project_id="aserta-mx-prd-warehousing",
            dataset_id="mi_dataset",
            table_id="mi_tabla",
            if_exists="replace"
        )
        
        # 4. Retornar respuesta
        return jsonify({
            "message": "Proceso completado exitosamente",
            "data": df.head(10).to_dict(orient='records')  # Primeras 10 filas
        })
    except Exception as e:
        return jsonify({
            "message": f"Error: {str(e)}",
            "data": None
        }), 500
```

#### 1.4 Buenas Pr√°cticas

- ‚úÖ **Manejo de errores**: Usa `try/except` para capturar errores
- ‚úÖ **Logging**: Usa `print()` para mensajes de depuraci√≥n (aparecer√°n en los logs de Cloud Run)
- ‚úÖ **Respuestas claras**: Devuelve mensajes descriptivos en el JSON
- ‚úÖ **Datos de muestra**: Incluye una muestra de los datos procesados en la respuesta

---

### Paso 2: Configurar Variables de Entorno (`local_env.yaml`)

#### 2.1 ¬øQu√© son las Variables de Entorno?

Son valores de configuraci√≥n que pueden cambiar entre entornos (local vs producci√≥n). Ejemplos:
- Rutas de archivos en Cloud Storage
- IDs de proyectos de GCP
- Nombres de tablas de BigQuery
- Versiones de APIs

#### 2.2 Configurar `local_env.yaml`

Edita el archivo `local_env.yaml` con tus variables:

```yaml
# Ejemplo de configuraci√≥n
ARCHIVO_EXCEL: gs://mi-bucket/datos/archivo.xlsx
PROJECT_ID: aserta-mx-prd-warehousing
DATASET_ID: mi_dataset
TABLA_DESTINO: mi_tabla
VERSION: 1.0.0
```

#### 2.3 Usar Variables en el C√≥digo

En `main.py`, accede a las variables as√≠:

```python
archivo = os.getenv('ARCHIVO_EXCEL')
project_id = os.getenv('PROJECT_ID', 'default_project')  # Con valor por defecto
```

> **Nota**: Si tu servicio no necesita variables de entorno, puedes dejar `local_env.yaml` vac√≠o o con solo `VERSION: 1.0.0`

---

### Paso 3: Configurar Dependencias (`requirements.txt`)

#### 3.1 ¬øQu√© es `requirements.txt`?

Este archivo lista todas las librer√≠as de Python que necesita tu c√≥digo. Cloud Run las instalar√° autom√°ticamente.

#### 3.2 Agregar Librer√≠as

El archivo base ya incluye las librer√≠as m√°s comunes. Agrega las que falten:

```txt
functions-framework==3.*
Flask
requests
gunicorn
openpyxl
pandas
numpy
google-cloud-storage
google-cloud-bigquery
tqdm
ipykernel
gcsfs
pandas-gbq
pyarrow
fastapi
uvicorn
pydantic
tabulate
looker-sdk

# Agrega aqu√≠ tus librer√≠as adicionales
# Ejemplo:
# scikit-learn==1.3.0
# matplotlib==3.7.0
```

#### 3.3 Verificar Librer√≠as del Notebook

1. Revisa las celdas `import` de tu notebook
2. Compara con `requirements.txt`
3. Agrega las que falten

> **Importante**: El archivo debe llamarse exactamente `requirements.txt` (no `requeriments.txt` ni otro nombre)

---

### Paso 4: Actualizar `Tools.py` (si es necesario)

#### 4.1 ¬øCu√°ndo actualizar `Tools.py`?

Solo si:
- Tu notebook usa funciones personalizadas que no est√°n en `Tools.py`
- Necesitas agregar nuevas funciones auxiliares
- Hay cambios en las funciones existentes

#### 4.2 Funciones Disponibles en `Tools.py`

- `upload_to_bigquery()`: Subir DataFrames a BigQuery
- `query_to_dataframe()`: Ejecutar queries SQL y obtener DataFrames
- `download_from_storage()`: Descargar archivos de Cloud Storage
- `upload_to_storage()`: Subir archivos a Cloud Storage

Revisa el archivo `Tools.py` para ver todas las funciones disponibles.

---

### Paso 5: Probar Localmente

#### 5.1 Activar el Entorno Virtual

```bash
# Desde la ra√≠z del proyecto
source .venv/bin/activate
```

#### 5.2 Instalar Dependencias Localmente

```bash
cd api_mi_proceso
pip install -r requirements.txt
```

#### 5.3 Ejecutar el Servicio Local

```bash
python local_main.py
```

Deber√≠as ver un mensaje como:
```
Running on http://localhost:4911
```

> **Nota**: Aseg√∫rate de estar en el directorio del servicio (`api_mi_proceso`)

#### 5.4 Probar el Servicio

En una **nueva terminal** (sin cerrar la anterior):

```bash
cd api_mi_proceso
source ./local_test.sh
```

O manualmente:
```bash
curl -X POST http://localhost:4911/ \
     -H "Content-Type: application/json" \
     -d '{}'
```

#### 5.5 Personalizar `local_test.sh`

Si tu servicio necesita par√°metros, edita `local_test.sh`:

```bash
echo "Testing local main"
curl -X POST http://localhost:4911/ \
     -H "Content-Type: application/json" \
     -d '{"parametro1": "valor1", "parametro2": "valor2"}'
```

#### 5.6 Verificar la Respuesta

Deber√≠as recibir un JSON como:
```json
{
  "message": "executed successfully",
  "data": [...]
}
```

Si hay errores, rev√≠salos en la terminal donde ejecutaste `local_main.py`.

---

### Paso 6: Desplegar a Cloud Run

#### 6.1 Preparaci√≥n

1. Aseg√∫rate de estar en el directorio `src`:
   ```bash
   cd ~/Aserta/aserta-dwh/src
   ```

2. Verifica que tu servicio est√© listo:
   - ‚úÖ `main.py` funciona localmente
   - ‚úÖ `requirements.txt` tiene todas las dependencias
   - ‚úÖ `local_env.yaml` est√° configurado

#### 6.2 Ejecutar el Despliegue

```bash
source ./deploy.sh ./api_mi_proceso nombre_funcion_cloudrun
```

**Par√°metros:**
- `./api_mi_proceso`: Ruta relativa a tu carpeta del servicio
- `nombre_funcion_cloudrun`: Nombre que tendr√° tu funci√≥n en GCP (ej: `carga_presupuesto_bq`)

**Ejemplo completo:**
```bash
source ./deploy.sh ./api_carga_presupuesto carga_presupuesto_bq
```

#### 6.3 ¬øQu√© hace el Script de Despliegue?

1. ‚úÖ Valida que exista el directorio del servicio
2. ‚úÖ Crea un archivo ZIP con todos los archivos necesarios
3. ‚úÖ Sube el ZIP a Cloud Storage
4. ‚úÖ Despliega la funci√≥n en Cloud Run
5. ‚úÖ Configura las variables de entorno desde `local_env.yaml`
6. ‚úÖ Limpia archivos temporales

#### 6.4 Verificar el Despliegue

Despu√©s del despliegue, ver√°s un mensaje con la URL de tu funci√≥n:
```
https://us-east4-aserta-dev-dwh.cloudfunctions.net/nombre_funcion_cloudrun
```

Puedes probarla con:
```bash
curl -X POST https://us-east4-aserta-dev-dwh.cloudfunctions.net/nombre_funcion_cloudrun \
     -H "Content-Type: application/json" \
     -d '{}'
```

---

## üîß Troubleshooting (Soluci√≥n de Problemas)

### Error: "ModuleNotFoundError: No module named 'pandas'"

**Causa**: La librer√≠a no est√° en `requirements.txt` o el archivo tiene un nombre incorrecto.

**Soluci√≥n**:
1. Verifica que el archivo se llame exactamente `requirements.txt`
2. Agrega la librer√≠a faltante al archivo
3. Vuelve a desplegar

### Error: "FileNotFoundError: No such file or directory"

**Causa**: Ruta de archivo incorrecta o archivo no existe en Cloud Storage.

**Soluci√≥n**:
1. Verifica que la ruta en `local_env.yaml` sea correcta
2. Aseg√∫rate de que el archivo exista en Cloud Storage
3. Verifica permisos de acceso

### Error: "Permission denied" o "403 Forbidden"

**Causa**: Falta de permisos en GCP.

**Soluci√≥n**:
1. Verifica que tengas permisos de Cloud Run Admin
2. Verifica permisos de Cloud Storage y BigQuery
3. Contacta al administrador de GCP

### El servicio funciona localmente pero falla en Cloud Run

**Causa**: Variables de entorno no configuradas o rutas incorrectas.

**Soluci√≥n**:
1. Verifica que `local_env.yaml` tenga todas las variables necesarias
2. Revisa los logs de Cloud Run en la consola de GCP
3. Aseg√∫rate de que las rutas usen rutas de GCS (gs://) y no rutas locales

### Error: "Timeout" en Cloud Run

**Causa**: El proceso tarda m√°s de 10 minutos (600 segundos).

**Soluci√≥n**:
1. Optimiza tu c√≥digo para que sea m√°s r√°pido
2. Si es necesario, contacta al equipo para aumentar el timeout
3. Considera dividir el proceso en pasos m√°s peque√±os

---

## üìù Checklist de Despliegue

Antes de desplegar, verifica:

- [ ] El c√≥digo funciona correctamente en el notebook
- [ ] `main.py` contiene toda la l√≥gica necesaria
- [ ] `requirements.txt` tiene todas las librer√≠as
- [ ] `local_env.yaml` est√° configurado correctamente
- [ ] El servicio funciona localmente (`python local_main.py`)
- [ ] Las pruebas locales pasan (`source ./local_test.sh`)
- [ ] No hay archivos de prueba o temporales en la carpeta
- [ ] El nombre del servicio es descriptivo y claro

---

## üí° Mejores Pr√°cticas

### C√≥digo

1. **Manejo de errores**: Siempre usa `try/except` para capturar errores
2. **Logging**: Usa `print()` para mensajes importantes (aparecen en logs de Cloud Run)
3. **Validaci√≥n**: Valida datos de entrada antes de procesarlos
4. **Respuestas claras**: Devuelve mensajes descriptivos en caso de error

### Variables de Entorno

1. **No hardcodear valores**: Usa variables de entorno para configuraci√≥n
2. **Valores por defecto**: Proporciona valores por defecto cuando sea posible
3. **Documentaci√≥n**: Documenta qu√© hace cada variable

### Despliegue

1. **Probar localmente primero**: Siempre prueba localmente antes de desplegar
2. **Versiones**: Usa versiones espec√≠ficas en `requirements.txt` cuando sea posible
3. **Nombres descriptivos**: Usa nombres claros para tus servicios

---

## üìö Recursos Adicionales

### Documentaci√≥n Oficial

- [Cloud Run Documentation](https://cloud.google.com/run/docs)
- [Functions Framework for Python](https://github.com/GoogleCloudPlatform/functions-framework-python)
- [BigQuery Python Client](https://cloud.google.com/bigquery/docs/reference/libraries)

### Soporte

Si tienes dudas o problemas:
1. Revisa los logs de Cloud Run en la consola de GCP
2. Consulta con el equipo de datos
3. Revisa ejemplos de otros servicios desplegados

---

## üéØ Ejemplo Completo

### Caso: Cargar datos de Excel a BigQuery

**1. Notebook Original:**
```python
import pandas as pd
df = pd.read_excel("datos.xlsx")
df.to_gbq("mi_dataset.mi_tabla", project_id="mi_proyecto")
```

**2. `main.py`:**
```python
import os
from flask import jsonify
import pandas as pd
import functions_framework
from Tools import upload_to_bigquery

@functions_framework.http
def functionRun(request):
    try:
        archivo = os.getenv('ARCHIVO_EXCEL')
        df = pd.read_excel(archivo)
        
        upload_to_bigquery(
            df,
            project_id=os.getenv('PROJECT_ID'),
            dataset_id=os.getenv('DATASET_ID'),
            table_id=os.getenv('TABLA_DESTINO'),
            if_exists="replace"
        )
        
        return jsonify({
            "message": f"Cargados {len(df)} registros exitosamente",
            "data": df.head(5).to_dict(orient='records')
        })
    except Exception as e:
        return jsonify({
            "message": f"Error: {str(e)}",
            "data": None
        }), 500
```

**3. `local_env.yaml`:**
```yaml
ARCHIVO_EXCEL: gs://mi-bucket/datos.xlsx
PROJECT_ID: aserta-mx-prd-warehousing
DATASET_ID: mi_dataset
TABLA_DESTINO: mi_tabla
VERSION: 1.0.0
```

**4. Desplegar:**
```bash
cd ~/Insaite/aserta-dwh/src
source ./deploy.sh ./api_carga_excel carga_excel_bq
```

---

## ‚úÖ ¬°Listo!

Con esta gu√≠a deber√≠as poder convertir cualquier notebook en un servicio de Cloud Run. Recuerda: **siempre prueba localmente primero** antes de desplegar.

¬°√âxito con tu despliegue! üöÄ
